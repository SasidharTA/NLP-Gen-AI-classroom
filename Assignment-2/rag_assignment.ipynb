{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16af32d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sasidhar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSVs...\n",
      "Building TF-IDF...\n",
      "Building BM25...\n",
      "Building dense embeddings + FAISS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 262.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BM25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 194.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Dense...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 45.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Hybrid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:30<00:00, 33.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved retrieval_summary.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "assignment_retriever_eval.py\n",
    "\n",
    "Usage:\n",
    "    python assignment_retriever_eval.py --qa_csv Squad_v2_dataset.csv --ctx_csv Context.csv --out_dir reports --sample 1000 --do_reader\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# --------------------\n",
    "# Helpers / Metrics\n",
    "# --------------------\n",
    "def normalize_answer(s: str) -> str:\n",
    "    import re, string\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = ''.join(ch for ch in s if ch not in set(string.punctuation))\n",
    "    return s.strip()\n",
    "\n",
    "def f1_score(pred: str, golds: List[str]) -> float:\n",
    "    p = normalize_answer(pred).split()\n",
    "    if len(p) == 0:\n",
    "        return 0.0\n",
    "    best = 0.0\n",
    "    for g in golds:\n",
    "        g_toks = normalize_answer(g).split()\n",
    "        common = set(p) & set(g_toks)\n",
    "        if not common:\n",
    "            continue\n",
    "        prec = len(common) / len(p)\n",
    "        rec = len(common) / max(1, len(g_toks))\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        if f1 > best:\n",
    "            best = f1\n",
    "    return best\n",
    "\n",
    "def exact_match(pred: str, golds: List[str]) -> float:\n",
    "    p = normalize_answer(pred)\n",
    "    for g in golds:\n",
    "        if p == normalize_answer(g):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def recall_at_k_single(retrieved_ids: List[str], gold_id: str) -> int:\n",
    "    return 1 if gold_id in retrieved_ids else 0\n",
    "\n",
    "def mrr_at_k_single(retrieved_ids: List[str], gold_id: str) -> float:\n",
    "    for i, cid in enumerate(retrieved_ids, start=1):\n",
    "        if cid == gold_id:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def dcg(rels: List[int]) -> float:\n",
    "    return sum((2**r - 1) / math.log2(i + 1) for i, r in enumerate(rels, start=1))\n",
    "\n",
    "def ndcg_at_k_single(retrieved_ids: List[str], gold_id: str, k: int) -> float:\n",
    "    rels = [1 if cid == gold_id else 0 for cid in retrieved_ids[:k]]\n",
    "    ideal = sorted(rels, reverse=True)\n",
    "    denom = dcg(ideal)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return dcg(rels) / denom\n",
    "\n",
    "# --------------------\n",
    "# Load Data\n",
    "# --------------------\n",
    "def parse_answers_field(x):\n",
    "    # field 'answers' may be a JSON string or plain string; normalize to list[str]\n",
    "    if pd.isna(x):\n",
    "        return [\"\"]\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        # try parse JSON\n",
    "        try:\n",
    "            obj = json.loads(x)\n",
    "            if isinstance(obj, dict) and \"text\" in obj:\n",
    "                t = obj[\"text\"]\n",
    "                return t if isinstance(t, list) else [t]\n",
    "            if isinstance(obj, list):\n",
    "                return obj\n",
    "        except Exception:\n",
    "            pass\n",
    "        # fallback: return string wrapped\n",
    "        return [x]\n",
    "    return [str(x)]\n",
    "\n",
    "# --------------------\n",
    "# Retrievers: TF-IDF, BM25, Dense (FAISS), Hybrid (RRF)\n",
    "# --------------------\n",
    "def build_tfidf(contexts: List[str]):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=100_000, stop_words='english')\n",
    "    X = tfidf.fit_transform(contexts)\n",
    "    return tfidf, X\n",
    "\n",
    "def tfidf_retrieve(query: str, tfidf, X, context_ids, contexts, top_k=5):\n",
    "    from sklearn.metrics.pairwise import linear_kernel\n",
    "    qv = tfidf.transform([query])\n",
    "    scores = linear_kernel(qv, X).ravel()\n",
    "    idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(context_ids[i], contexts[i], float(scores[i])) for i in idx]\n",
    "\n",
    "def build_bm25(tokenized_contexts: List[List[str]]):\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    return BM25Okapi(tokenized_contexts)\n",
    "\n",
    "def bm25_retrieve(query: str, bm25, context_ids, contexts, top_k=5):\n",
    "    import nltk\n",
    "    q_tokens = nltk.word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(q_tokens)\n",
    "    idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(context_ids[i], contexts[i], float(scores[i])) for i in idx]\n",
    "\n",
    "def build_dense(contexts: List[str], embed_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(embed_model_name)\n",
    "    emb = model.encode(contexts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    import faiss\n",
    "    dim = emb.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(emb.astype('float32'))\n",
    "    return model, emb, index\n",
    "\n",
    "def dense_retrieve(query: str, model, index, context_ids, contexts, top_k=5):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "    scores, idx = index.search(q_emb, top_k)\n",
    "    idx = idx[0]; scores = scores[0]\n",
    "    return [(context_ids[i], contexts[i], float(scores[j])) for j,i in enumerate(idx)]\n",
    "\n",
    "def rrf_fuse(list_of_ranked: List[List[Tuple[str,str,float]]], k=5, rrf_k=60):\n",
    "    # Reciprocal Rank Fusion across lists of (cid, text, score)\n",
    "    from collections import defaultdict\n",
    "    rr = defaultdict(float)\n",
    "    info = {}\n",
    "    for L in list_of_ranked:\n",
    "        for rank, item in enumerate(L, start=1):\n",
    "            cid = item[0]\n",
    "            rr[cid] += 1.0 / (rrf_k + rank)\n",
    "            if cid not in info:\n",
    "                info[cid] = item\n",
    "    fused = [(cid, info[cid][1], rr[cid]) for cid in rr]\n",
    "    fused.sort(key=lambda x: x[2], reverse=True)\n",
    "    return fused[:k]\n",
    "\n",
    "# --------------------\n",
    "# Reranker (Cross-Encoder)\n",
    "# --------------------\n",
    "def build_reranker(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    return CrossEncoder(model_name)\n",
    "\n",
    "def cross_rerank(query: str, candidates: List[Tuple[str,str,float]], reranker, top_k=5):\n",
    "    pairs = [(query, text) for (cid, text, s) in candidates]\n",
    "    if len(pairs) == 0:\n",
    "        return []\n",
    "    scores = reranker.predict(pairs, show_progress_bar=False)\n",
    "    cand_scored = [(cid, text, float(score)) for (cid, text, s), score in zip(candidates, scores)]\n",
    "    cand_scored.sort(key=lambda x: x[2], reverse=True)\n",
    "    return cand_scored[:top_k]\n",
    "\n",
    "# --------------------\n",
    "# Extractive reader (optional)\n",
    "# --------------------\n",
    "def build_reader(model_name=\"deepset/roberta-base-squad2\", device=\"cpu\"):\n",
    "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "    import torch\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    mdl.to(device)\n",
    "    return tok, mdl, device\n",
    "\n",
    "def extractive_answer(question: str, context: str, tok, mdl, device=\"cpu\", max_len=512):\n",
    "    import torch\n",
    "    inputs = tok.encode_plus(question, context, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = mdl(**inputs)\n",
    "    start = int(out.start_logits.argmax(dim=1).cpu().numpy()[0])\n",
    "    end = int(out.end_logits.argmax(dim=1).cpu().numpy()[0])\n",
    "    if end < start:\n",
    "        return \"\"\n",
    "    ids = inputs[\"input_ids\"][0][start:end+1].cpu().numpy().tolist()\n",
    "    ans = tok.decode(ids, skip_special_tokens=True)\n",
    "    return ans.strip()\n",
    "\n",
    "# --------------------\n",
    "# Evaluate retriever\n",
    "# --------------------\n",
    "def evaluate_retriever(df: pd.DataFrame, retriever_fn, retriever_name: str, topk_list=[1,3,5,10,20], limit=None, reranker=None):\n",
    "    metrics = {k: {\"recall_sum\":0.0, \"mrr_sum\":0.0, \"ndcg_sum\":0.0, \"count\":0} for k in topk_list}\n",
    "    n = limit or len(df)\n",
    "    for i, row in enumerate(tqdm(df.itertuples(index=False), total=n)):\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "        question = getattr(row, \"question\")\n",
    "        gold = getattr(row, \"context_id\")\n",
    "        # retrieve larger list for potential rerank\n",
    "        topn = max(topk_list) * 5\n",
    "        candidates = retriever_fn(question, top_k=topn)\n",
    "        for k in topk_list:\n",
    "            cand_k = candidates[:k]\n",
    "            if reranker is not None:\n",
    "                cand_k = reranker(question, candidates, top_k=k)\n",
    "            retrieved_ids = [cid for cid, _, _ in cand_k]\n",
    "            metrics[k][\"recall_sum\"] += recall_at_k_single(retrieved_ids, gold)\n",
    "            metrics[k][\"mrr_sum\"] += mrr_at_k_single(retrieved_ids, gold)\n",
    "            metrics[k][\"ndcg_sum\"] += ndcg_at_k_single(retrieved_ids, gold, k)\n",
    "            metrics[k][\"count\"] += 1\n",
    "    rows = []\n",
    "    for k in topk_list:\n",
    "        cnt = metrics[k][\"count\"] or 1\n",
    "        rows.append({\n",
    "            \"retriever\": retriever_name,\n",
    "            \"top_k\": k,\n",
    "            \"recall\": metrics[k][\"recall_sum\"] / cnt,\n",
    "            \"mrr\": metrics[k][\"mrr_sum\"] / cnt,\n",
    "            \"ndcg\": metrics[k][\"ndcg_sum\"] / cnt,\n",
    "            \"count\": cnt\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --------------------\n",
    "# Main CLI\n",
    "# --------------------\n",
    "def main(args):\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    print(\"Loading CSVs...\")\n",
    "    qa_columns = [\"question\", \"answers\", \"context_id\"]\n",
    "    ctx_columns = [\"context_id\", \"context\"]\n",
    "    qa_df = pd.read_csv(args.qa_csv,header=None,names=qa_columns,encoding=\"latin1\")\n",
    "    ctx_df = pd.read_csv(args.ctx_csv,header=None,names=ctx_columns,encoding=\"latin1\")\n",
    "\n",
    "    qa_df['answers_list'] = qa_df['answers'].apply(parse_answers_field)\n",
    "    data = qa_df.merge(ctx_df, left_on='context_id', right_on='context_id', how='left')\n",
    "    #assert data['context'].notnull().all(), \"Some context_id missing in Context.csv\"\n",
    "\n",
    "    # optionally sample\n",
    "    eval_df = data[['question','context_id','answers_list']].copy()\n",
    "    if args.sample and args.sample < len(eval_df):\n",
    "        eval_df = eval_df.sample(n=args.sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    contexts = ctx_df['context'].astype(str).tolist()\n",
    "    context_ids = ctx_df['context_id'].astype(str).tolist()\n",
    "\n",
    "    # Build TF-IDF\n",
    "    print(\"Building TF-IDF...\")\n",
    "    tfidf, X_tfidf = build_tfidf(contexts)\n",
    "    tfidf_fn = lambda q, top_k: tfidf_retrieve(q, tfidf, X_tfidf, context_ids, contexts, top_k=top_k)\n",
    "\n",
    "    # Build BM25\n",
    "    print(\"Building BM25...\")\n",
    "    tokenized_contexts = [nltk.word_tokenize(c.lower()) for c in contexts]\n",
    "    bm25 = build_bm25(tokenized_contexts)\n",
    "    bm25_fn = lambda q, top_k: bm25_retrieve(q, bm25, context_ids, contexts, top_k=top_k)\n",
    "\n",
    "    # Build Dense + FAISS\n",
    "    print(\"Building dense embeddings + FAISS...\")\n",
    "    embed_model_name = args.embed_model\n",
    "    dense_model, dense_embs, faiss_index = None, None, None\n",
    "    dense_model, dense_embs, faiss_index = build_dense(contexts, embed_model_name)\n",
    "    dense_fn = lambda q, top_k: dense_retrieve(q, dense_model, faiss_index, context_ids, contexts, top_k=top_k)\n",
    "\n",
    "    # Hybrid (BM25 + Dense via RRF)\n",
    "    hybrid_fn = lambda q, top_k: rrf_fuse([bm25_fn(q, top_k=50), dense_fn(q, top_k=50)], k=top_k)\n",
    "\n",
    "    # Optional reranker\n",
    "    reranker = None\n",
    "    rerank_fn = None\n",
    "    if args.do_rerank:\n",
    "        print(\"Loading cross-encoder reranker...\")\n",
    "        reranker = build_reranker(args.reranker_model)\n",
    "        rerank_fn = lambda q, candidates, top_k: cross_rerank(q, candidates, reranker, top_k=top_k)\n",
    "\n",
    "    # Evaluate retrievers\n",
    "    topk_list = [1,3,5,10,20]\n",
    "    print(\"Evaluating TF-IDF...\")\n",
    "    res_tfidf = evaluate_retriever(eval_df, tfidf_fn, \"tfidf\", topk_list=topk_list, limit=None, reranker=rerank_fn)\n",
    "    print(\"Evaluating BM25...\")\n",
    "    res_bm25  = evaluate_retriever(eval_df, bm25_fn, \"bm25\", topk_list=topk_list, limit=None, reranker=rerank_fn)\n",
    "    print(\"Evaluating Dense...\")\n",
    "    res_dense = evaluate_retriever(eval_df, dense_fn, \"dense\", topk_list=topk_list, limit=None, reranker=rerank_fn)\n",
    "    print(\"Evaluating Hybrid...\")\n",
    "    res_hybrid = evaluate_retriever(eval_df, hybrid_fn, \"hybrid\", topk_list=topk_list, limit=None, reranker=rerank_fn)\n",
    "\n",
    "    summary = pd.concat([res_tfidf, res_bm25, res_dense, res_hybrid], ignore_index=True)\n",
    "    summary.to_csv(os.path.join(args.out_dir, \"retrieval_summary.csv\"), index=False)\n",
    "    print(\"Saved retrieval_summary.csv\")\n",
    "\n",
    "    # Optional: answer extraction evaluation on small sample\n",
    "    if args.do_reader:\n",
    "        print(\"Building extractive reader...\")\n",
    "        import torch\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tok, mdl, dev = build_reader(args.reader_model, device)\n",
    "        sample_n = min(args.reader_sample, len(eval_df))\n",
    "        rows = []\n",
    "        for row in tqdm(eval_df.head(sample_n).itertuples(index=False), total=sample_n):\n",
    "            q = row.question\n",
    "            golds = row.answers_list\n",
    "            candidates = dense_fn(q, top_k=args.ans_topk)\n",
    "            if rerank_fn:\n",
    "                candidates = rerank_fn(q, candidates, top_k=args.ans_topk)\n",
    "            concat_ctx = \"\\n\\n\".join([t for (_cid, t, _s) in candidates])\n",
    "            pred = extractive_answer(q, concat_ctx, tok, mdl, device=device)\n",
    "            em = exact_match(pred, golds)\n",
    "            f1 = f1_score(pred, golds)\n",
    "            rows.append({\"question\": q, \"pred\": pred, \"em\": em, \"f1\": f1})\n",
    "        ans_df = pd.DataFrame(rows)\n",
    "        ans_df.to_csv(os.path.join(args.out_dir, \"answer_eval_sample.csv\"), index=False)\n",
    "        print(\"Saved answer_eval_sample.csv\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--qa_csv\", type=str, default=\"data/squad_v2_dataset.csv\")\n",
    "    parser.add_argument(\"--ctx_csv\", type=str, default=\"data/context.csv\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"reports\")\n",
    "    parser.add_argument(\"--sample\", type=int, default=1000, help=\"sample size for evaluation (None for full)\")\n",
    "    parser.add_argument(\"--embed_model\", type=str, default=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    parser.add_argument(\"--do_rerank\", action=\"store_true\", help=\"use cross-encoder reranker\")\n",
    "    parser.add_argument(\"--reranker_model\", type=str, default=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    parser.add_argument(\"--do_reader\", action=\"store_true\", help=\"run extractive reader on small sample\")\n",
    "    parser.add_argument(\"--reader_model\", type=str, default=\"deepset/roberta-base-squad2\")\n",
    "    parser.add_argument(\"--reader_sample\", type=int, default=200)\n",
    "    parser.add_argument(\"--ans_topk\", type=int, default=5)\n",
    "    args, unknown = parser.parse_known_args()  # ignore unknown args\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6e3d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom/Assignment-2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c320043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%chdir` not found.\n"
     ]
    }
   ],
   "source": [
    "%chdir '/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4433f3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom/.nlp-genai/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd '/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae3c6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/sasidhar.chennup/Documents/Tiger-Training/NLP-GenAI-Classroom/NLP-Gen-AI-classroom'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd16a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlp-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
